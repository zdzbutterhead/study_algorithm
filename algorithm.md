![Hello 算法](https://www.hello-algo.com/assets/covers/chapter_hello_algo.jpg)

## 第 0 章  前言

### 0.1  关于本书

![前言](https://www.hello-algo.com/assets/covers/chapter_preface.jpg)

算法犹如美妙的交响乐，每一行代码都像韵律般流淌。

![本书主要内容](https://www.hello-algo.com/chapter_preface/about_the_book.assets/hello_algo_mindmap.png)

### 0.2  如何使用本书

#### 0.2.1  行文风格约定

- 标题后标注 `*` 的是选读章节，内容相对困难。如果你的时间有限，可以先跳过。
- 专业术语会使用黑体（纸质版和 PDF 版）或添加下划线（网页版），例如数组（array）。建议记住它们，以便阅读文献。
- 重点内容和总结性语句会 **加粗**，这类文字值得特别关注。
- 有特指含义的词句会使用“引号”标注，以避免歧义。
- 当涉及编程语言之间不一致的名词时，本书均以 Python 为准，例如使用 `None` 来表示“空”。
- 本书部分放弃了编程语言的注释规范，以换取更加紧凑的内容排版。注释主要分为三种类型：标题注释、内容注释、多行注释。

#### 0.2.3  在代码实践中加深理解

本书的配套代码托管在 [GitHub 仓库](https://github.com/krahets/hello-algo)。如图 0-3 所示，**源代码附有测试样例，可一键运行**。

如果时间允许，**建议你参照代码自行敲一遍**。如果学习时间有限，请至少通读并运行所有代码。

与阅读代码相比，编写代码的过程往往能带来更多收获。**动手学，才是真的学**。

运行代码的前置工作主要分为三步。

**第一步：安装本地编程环境**。请参照附录所示的[教程](https://www.hello-algo.com/chapter_appendix/installation/)进行安装，如果已安装，则可跳过此步骤。

**第二步：克隆或下载代码仓库**。前往 [GitHub 仓库](https://github.com/krahets/hello-algo)。如果已经安装 [Git](https://git-scm.com/downloads) ，可以通过以下命令克隆本仓库：

```
git clone https://github.com/krahets/hello-algo.git
```

当然，你也可以在图 0-4 所示的位置，点击“Download ZIP”按钮直接下载代码压缩包，然后在本地解压即可。

[![克隆仓库与下载代码](https://www.hello-algo.com/chapter_preface/suggestions.assets/download_code.png)](https://www.hello-algo.com/chapter_preface/suggestions.assets/download_code.png)

图 0-4  克隆仓库与下载代码

**第三步：运行源代码**。如图 0-5 所示，对于顶部标有文件名称的代码块，我们可以在仓库的 `codes` 文件夹内找到对应的源代码文件。源代码文件可一键运行，将帮助你节省不必要的调试时间，让你能够专注于学习内容。

[![代码块与对应的源代码文件](https://www.hello-algo.com/chapter_preface/suggestions.assets/code_md_to_repo.png)](https://www.hello-algo.com/chapter_preface/suggestions.assets/code_md_to_repo.png)

图 0-5  代码块与对应的源代码文件

除了本地运行代码，**网页版还支持 Python 代码的可视化运行**（基于 [pythontutor](https://pythontutor.com/) 实现）。如图 0-6 所示，你可以点击代码块下方的“可视化运行”来展开视图，观察算法代码的执行过程；也可以点击“全屏观看”，以获得更好的阅览体验。

[![Python 代码的可视化运行](https://www.hello-algo.com/chapter_preface/suggestions.assets/pythontutor_example.png)](https://www.hello-algo.com/chapter_preface/suggestions.assets/pythontutor_example.png)

图 0-6  Python 代码的可视化运行

#### 0.2.5  算法学习路线

从总体上看，我们可以将学习数据结构与算法的过程划分为三个阶段。

1. **阶段一：算法入门**。我们需要熟悉各种数据结构的特点和用法，学习不同算法的原理、流程、用途和效率等方面的内容。
2. **阶段二：刷算法题**。建议从热门题目开刷，先积累至少 100 道题目，熟悉主流的算法问题。初次刷题时，“知识遗忘”可能是一个挑战，但请放心，这是很正常的。我们可以按照“艾宾浩斯遗忘曲线”来复习题目，通常在进行 3～5 轮的重复后，就能将其牢记在心。推荐的题单和刷题计划请见此 [GitHub 仓库](https://github.com/krahets/LeetCode-Book)。
3. **阶段三：搭建知识体系**。在学习方面，我们可以阅读算法专栏文章、解题框架和算法教材，以不断丰富知识体系。在刷题方面，可以尝试采用进阶刷题策略，如按专题分类、一题多解、一解多题等，相关的刷题心得可以在各个社区找到。

如图 0-8 所示，本书内容主要涵盖“阶段一”，旨在帮助你更高效地展开阶段二和阶段三的学习。

[![算法学习路线](https://www.hello-algo.com/chapter_preface/suggestions.assets/learning_route.png)](https://www.hello-algo.com/chapter_preface/suggestions.assets/learning_route.png)

图 0-8  算法学习路线

## 第 1 章  初识算法 

![初识算法](https://www.hello-algo.com/assets/covers/chapter_introduction.jpg)

### 1.1  算法无处不在

**例一：查字典**。在字典里，每个汉字都对应一个拼音，而字典是按照拼音字母顺序排列的。假设我们需要查找一个拼音首字母为 的字，通常会按照图 1-1 所示的方式实现。

1. 翻开字典约一半的页数，查看该页的首字母是什么，假设首字母为 。
2. 由于在拼音字母表中 位于 之后，所以排除字典前半部分，查找范围缩小到后半部分。
3. 不断重复步骤 `1.` 和步骤 `2.` ，直至找到拼音首字母为 的页码为止。

查字典这个小学生必备技能，实际上就是著名的“二分查找”算法。从数据结构的角度，我们可以把字典视为一个已排序的“数组”；从算法的角度，我们可以将上述查字典的一系列操作看作“二分查找”。

**例二：整理扑克**。我们在打牌时，每局都需要整理手中的扑克牌，使其从小到大排列，实现流程如图 1-2 所示。

1. 将扑克牌划分为“有序”和“无序”两部分，并假设初始状态下最左 1 张扑克牌已经有序。
2. 在无序部分抽出一张扑克牌，插入至有序部分的正确位置；完成后最左 2 张扑克已经有序。
3. 不断循环步骤 `2.` ，每一轮将一张扑克牌从无序部分插入至有序部分，直至所有扑克牌都有序。

[![扑克排序步骤](https://www.hello-algo.com/chapter_introduction/algorithms_are_everywhere.assets/playing_cards_sorting.png)](https://www.hello-algo.com/chapter_introduction/algorithms_are_everywhere.assets/playing_cards_sorting.png)

图 1-2  扑克排序步骤

上述整理扑克牌的方法本质上是“插入排序”算法，它在处理小型数据集时非常高效。许多编程语言的排序库函数中都有插入排序的身影。

**例三：货币找零**。假设我们在超市购买了 元的商品，给了收银员 元，则收银员需要找我们 元。他会很自然地完成如图 1-3 所示的思考。

1. 可选项是比31元面值更小的货币，包括1元、5元、10元、20元。
2. 从可选项中拿出最大的20元，剩余31-20=11元。
3. 从剩余可选项中拿出最大的10元，剩余11-10=1元。
4. 从剩余可选项中拿出最大的1元，剩余1-1=0元。
5. 完成找零，方案为20+10+1=31元。

[![货币找零过程](https://www.hello-algo.com/chapter_introduction/algorithms_are_everywhere.assets/greedy_change.png)](https://www.hello-algo.com/chapter_introduction/algorithms_are_everywhere.assets/greedy_change.png)

图 1-3  货币找零过程

在以上步骤中，我们每一步都采取当前看来最好的选择（尽可能用大面额的货币），最终得到了可行的找零方案。从数据结构与算法的角度看，这种方法本质上是“贪心”算法。

### 1.2  算法是什么

#### 1.2.1  算法定义

算法（algorithm）是在有限时间内解决特定问题的一组指令或操作步骤，它具有以下特性。

- 问题是明确的，包含清晰的输入和输出定义。
- 具有可行性，能够在有限步骤、时间和内存空间下完成。
- 各步骤都有确定的含义，在相同的输入和运行条件下，输出始终相同。

#### 1.2.2  数据结构定义

数据结构（data structure）是组织和存储数据的方式，涵盖数据内容、数据之间关系和数据操作方法，它具有以下设计目标。

- 空间占用尽量少，以节省计算机内存。
- 数据操作尽可能快速，涵盖数据访问、添加、删除、更新等。
- 提供简洁的数据表示和逻辑信息，以便算法高效运行。

**数据结构设计是一个充满权衡的过程**。如果想在某方面取得提升，往往需要在另一方面作出妥协。下面举两个例子。

- 链表相较于数组，在数据添加和删除操作上更加便捷，但牺牲了数据访问速度。
- 图相较于链表，提供了更丰富的逻辑信息，但需要占用更大的内存空间。

#### 1.2.3  数据结构与算法的关系

- 数据结构是算法的基石。数据结构为算法提供了结构化存储的数据，以及操作数据的方法。
- 算法为数据结构注入生命力。数据结构本身仅存储数据信息，结合算法才能解决特定问题。
- 算法通常可以基于不同的数据结构实现，但执行效率可能相差很大，选择合适的数据结构是关键。

[![数据结构与算法的关系](https://www.hello-algo.com/chapter_introduction/what_is_dsa.assets/relationship_between_data_structure_and_algorithm.png)](https://www.hello-algo.com/chapter_introduction/what_is_dsa.assets/relationship_between_data_structure_and_algorithm.png)

图 1-4  数据结构与算法的关系

数据结构与算法犹如拼装积木。一套积木，除了包含许多零件之外，还附有详细的组装说明书。我们按照说明书一步步操作，就能组装出精美的积木模型。

两者的详细对应关系如表 1-1 所示。

表 1-1  将数据结构与算法类比为拼装积木

| 数据结构与算法 | 拼装积木                                 |
| :------------- | :--------------------------------------- |
| 输入数据       | 未拼装的积木                             |
| 数据结构       | 积木组织形式，包括形状、大小、连接方式等 |
| 算法           | 把积木拼成目标形态的一系列操作步骤       |
| 输出数据       | 积木模型                                 |

值得说明的是，数据结构与算法是独立于编程语言的。正因如此，本书得以提供基于多种编程语言的实现。

在实际讨论时，我们通常会将“数据结构与算法”简称为“算法”。比如众所周知的 LeetCode 算法题目，实际上同时考查数据结构和算法两方面的知识。

我认为学算法（以及其他基础科目）的意义不是在于在工作中从零实现它，而是基于学到的知识，在解决问题时能够作出专业的反应和判断，从而提升工作的整体质量。

在工程领域中，大量问题是难以达到最优解的，许多问题只是被“差不多”地解决了。问题的难易程度一方面取决于问题本身的性质，另一方面也取决于观测问题的人的知识储备。人的知识越完备、经验越多，分析问题就会越深入，问题就能被解决得更优雅。

## 第 2 章  复杂度分析

![复杂度分析](https://www.hello-algo.com/assets/covers/chapter_complexity_analysis.jpg)

（第一眼以为三体的封面）

### 2.1  算法效率评估

在算法设计中，我们先后追求以下两个层面的目标。

1. **找到问题解法**：算法需要在规定的输入范围内可靠地求得问题的正确解。
2. **寻求最优解法**：同一个问题可能存在多种解法，我们希望找到尽可能高效的算法。

也就是说，在能够解决问题的前提下，算法效率已成为衡量算法优劣的主要评价指标，它包括以下两个维度。

- **时间效率**：算法运行时间的长短。
- **空间效率**：算法占用内存空间的大小。

效率评估方法主要分为两种：实际测试、理论估算。

#### 2.1.1  实际测试

假设我们现在有算法 `A` 和算法 `B` ，它们都能解决同一问题，现在需要对比这两个算法的效率。最直接的方法是找一台计算机，运行这两个算法，并监控记录它们的运行时间和内存占用情况。这种评估方式能够反映真实情况，但也存在较大的局限性。

一方面，**难以排除测试环境的干扰因素**。硬件配置会影响算法的性能表现。比如一个算法的并行度较高，那么它就更适合在多核 CPU 上运行，一个算法的内存操作密集，那么它在高性能内存上的表现就会更好。也就是说，算法在不同的机器上的测试结果可能是不一致的。这意味着我们需要在各种机器上进行测试，统计平均效率，而这是不现实的。

另一方面，**展开完整测试非常耗费资源**。随着输入数据量的变化，算法会表现出不同的效率。例如，在输入数据量较小时，算法 `A` 的运行时间比算法 `B` 短；而在输入数据量较大时，测试结果可能恰恰相反。因此，为了得到有说服力的结论，我们需要测试各种规模的输入数据，而这需要耗费大量的计算资源。

#### 2.1.2  理论估算

由于实际测试具有较大的局限性，我们可以考虑仅通过一些计算来评估算法的效率。这种估算方法被称为渐近复杂度分析（asymptotic complexity analysis），简称复杂度分析。

复杂度分析能够体现算法运行所需的时间和空间资源与输入数据大小之间的关系。**它描述了随着输入数据大小的增加，算法执行所需时间和空间的增长趋势**。这个定义有些拗口，我们可以将其分为三个重点来理解。

- “时间和空间资源”分别对应时间复杂度（time complexity）和空间复杂度（space complexity）。
- “随着输入数据大小的增加”意味着复杂度反映了算法运行效率与输入数据体量之间的关系。
- “时间和空间的增长趋势”表示复杂度分析关注的不是运行时间或占用空间的具体值，而是时间或空间增长的“快慢”。

**复杂度分析克服了实际测试方法的弊端**，体现在以下几个方面。

- 它无需实际运行代码，更加绿色节能。
- 它独立于测试环境，分析结果适用于所有运行平台。
- 它可以体现不同数据量下的算法效率，尤其是在大数据量下的算法性能。

#### Comment

作者认为：“时间效率”整体上指的就是算法在“时间”上的快慢，而非运行计算次数。还有人则说： 时间效率指算法运行的计算次数。

是的，原因很直接：我们提出时间效率这个概念，最终想要关心的是“时间”上的快慢，而不是“执行次数”。

执行次数更多和时间复杂度关联，它是反映时间效率的有效指标，但很有可能给出错误的结论。我们有时会遇到一种情况：算法 A 比算法 B 的时间复杂度更高（更差），但反而在给定数据下运行地更快（时间更短、效率更高）。一个典型的例子是插入排序 vs. 归并排序在数据量较小时的效率对比。

### 2.2  迭代与递归

在算法中，重复执行某个任务是很常见的，它与复杂度分析息息相关。因此，在介绍时间复杂度和空间复杂度之前，我们先来了解如何在程序中实现重复执行任务，即两种基本的程序控制结构：迭代、递归。

#### 2.2.1  迭代

迭代（iteration）是一种重复执行某个任务的控制结构。在迭代中，程序会在满足一定的条件下重复执行某段代码，直到这个条件不再满足。

1.  for 循环

`for` 循环是最常见的迭代形式之一，**适合在预先知道迭代次数时使用**。

2.  while 循环

与 `for` 循环类似，`while` 循环也是一种实现迭代的方法。在 `while` 循环中，程序每轮都会先检查条件，如果条件为真，则继续执行，否则就结束循环。

**`while` 循环比 `for` 循环的自由度更高**。在 `while` 循环中，我们可以自由地设计条件变量的初始化和更新步骤。

总的来说，**`for` 循环的代码更加紧凑，`while` 循环更加灵活**，两者都可以实现迭代结构。选择使用哪一个应该根据特定问题的需求来决定。

3. 嵌套循环

我们可以在一个循环结构内嵌套另一个循环结构

```python
def nested_for_loop(n: int) -> str:
    """双层 for 循环"""
    res = ""
    # 循环 i = 1, 2, ..., n-1, n
    for i in range(1, n + 1):
        # 循环 j = 1, 2, ..., n-1, n
        for j in range(1, n + 1):
            res += f"({i}, {j}), "
    return res
```

`res += f"({i}, {j}), "` 是一个使用 Python **f-strings**（格式化字符串）的赋值语句，用于动态构建字符串。

#### 2.2.2  递归

递归（recursion）是一种算法策略，通过函数调用自身来解决问题。它主要包含两个阶段。

1. **递**：程序不断深入地调用自身，通常传入更小或更简化的参数，直到达到“终止条件”。
2. **归**：触发“终止条件”后，程序从最深层的递归函数开始逐层返回，汇聚每一层的结果。

而从实现的角度看，递归代码主要包含三个要素。

1. **终止条件**：用于决定什么时候由“递”转“归”。
2. **递归调用**：对应“递”，函数调用自身，通常输入更小或更简化的参数。
3. **返回结果**：对应“归”，将当前递归层级的结果返回至上一层。

虽然从计算角度看，迭代与递归可以得到相同的结果，**但它们代表了两种完全不同的思考和解决问题的范式**。

- **迭代**：“自下而上”地解决问题。从最基础的步骤开始，然后不断重复或累加这些步骤，直到任务完成。
- **递归**：“自上而下”地解决问题。将原问题分解为更小的子问题，这些子问题和原问题具有相同的形式。接下来将子问题继续分解为更小的子问题，直到基本情况时停止（基本情况的解是已知的）。

1.  调用栈

递归函数每次调用自身时，系统都会为新开启的函数分配内存，以存储局部变量、调用地址和其他信息等。这将导致两方面的结果。

- 函数的上下文数据都存储在称为“栈帧空间”的内存区域中，直至函数返回后才会被释放。因此，**递归通常比迭代更加耗费内存空间**。
- 递归调用函数会产生额外的开销。**因此递归通常比循环的时间效率更低**。

在实际中，编程语言允许的递归深度通常是有限的，过深的递归可能导致栈溢出错误。

2. 尾递归

有趣的是，**如果函数在返回前的最后一步才进行递归调用**，则该函数可以被编译器或解释器优化，使其在空间效率上与迭代相当。这种情况被称为尾递归（tail recursion）。

- **普通递归**：当函数返回到上一层级的函数后，需要继续执行代码，因此系统需要保存上一层调用的上下文。
- **尾递归**：递归调用是函数返回前的最后一个操作，这意味着函数返回到上一层级后，无须继续执行其他操作，因此系统无须保存上一层函数的上下文。

```python
# 普通递归
def recur(n: int) -> int:
    """递归"""
    # 终止条件
    if n == 1:
        return 1
    # 递：递归调用
    res = recur(n - 1)
    # 归：返回结果
    return n + res
```

![求和函数的递归过程](https://www.hello-algo.com/chapter_computational_complexity/iteration_and_recursion.assets/recursion_sum.png)

```python
# 尾递归
def tail_recur(n, res):
    # 终止条件
    if n == 0:
        return res
    # 尾递归调用
    return tail_recur(n - 1, res + n)

# 不属于尾递归
def recursion(n:int)->int:
    if n == 1:
        return 1
    else:
        return n + recursion(n-1)
# 该递归函数不属于尾递归。尾递归的定义是：递归调用是函数的最后一个操作，且返回值直接传递给上层。而你的代码中，递归调用 recursion(n-1) 之后还需要执行加法操作 n + ...，因此不符合尾递归的条件。
'''
尾递归通常需要通过累加器参数来保存中间结果，避免递归返回后执行额外操作
累加器 acc：用于保存当前的累加和。
递归调用：直接返回 tail_recursion(...)，无后续操作。
'''
```

![尾递归过程](https://www.hello-algo.com/chapter_computational_complexity/iteration_and_recursion.assets/tail_recursion_sum.png)

> 请注意，许多编译器或解释器并不支持尾递归优化。例如，Python 默认不支持尾递归优化，因此即使函数是尾递归形式，仍然可能会遇到栈溢出问题。

3. 递归树

当处理与“分治”相关的算法问题时，递归往往比迭代的思路更加直观、代码更加易读。

```python
def fib(n: int) -> int:
    """斐波那契数列：递归"""
    # 终止条件 f(1) = 0, f(2) = 1
    if n == 1 or n == 2:
        return n - 1
    # 递归调用 f(n) = f(n-1) + f(n-2)
    res = fib(n - 1) + fib(n - 2)
    # 返回结果 f(n)
    return res
```

观察以上代码，我们在函数内递归调用了两个函数，**这意味着从一个调用产生了两个调用分支**。如图 2-6 所示，这样不断递归调用下去，最终将产生一棵层数为 的递归树（recursion tree）。

![斐波那契数列的递归树](https://www.hello-algo.com/chapter_computational_complexity/iteration_and_recursion.assets/recursion_tree.png)

图 2-6  斐波那契数列的递归树

递归的执行顺序是**深度优先、先左后右的 “后序处理”**

从本质上看，递归体现了“将问题分解为更小子问题”的思维范式，这种分治策略至关重要。

- 从算法角度看，搜索、排序、回溯、分治、动态规划等许多重要算法策略直接或间接地应用了这种思维方式。
- 从数据结构角度看，递归天然适合处理链表、树和图的相关问题，因为它们非常适合用分治思想进行分析。

#### 2.2.3  两者对比

表 2-1  迭代与递归特点对比

|          | 迭代                                   | 递归                                                         |
| :------- | :------------------------------------- | :----------------------------------------------------------- |
| 实现方式 | 循环结构                               | 函数调用自身                                                 |
| 时间效率 | 效率通常较高，无函数调用开销           | 每次函数调用都会产生开销                                     |
| 内存使用 | 通常使用固定大小的内存空间             | 累积函数调用可能使用大量的栈帧空间                           |
| 适用问题 | 适用于简单循环任务，代码直观、可读性好 | 适用于子问题分解，如树、图、分治、回溯等，代码结构简洁、清晰 |

那么，迭代和递归具有什么内在联系呢？以上述递归函数为例，求和操作在递归的“归”阶段进行。这意味着最初被调用的函数实际上是最后完成其求和操作的，**这种工作机制与栈的“先入后出”原则异曲同工**。

事实上，“调用栈”和“栈帧空间”这类递归术语已经暗示了递归与栈之间的密切关系。

1. **递**：当函数被调用时，系统会在“调用栈”上为该函数分配新的栈帧，用于存储函数的局部变量、参数、返回地址等数据。
2. **归**：当函数完成执行并返回时，对应的栈帧会被从“调用栈”上移除，恢复之前函数的执行环境。

尽管迭代和递归在很多情况下可以互相转化，但不一定值得这样做，有以下两点原因。

- 转化后的代码可能更加难以理解，可读性更差。
- 对于某些复杂问题，模拟系统调用栈的行为可能非常困难。

总之，**选择迭代还是递归取决于特定问题的性质**。在编程实践中，权衡两者的优劣并根据情境选择合适的方法至关重要。

#### Comment

事实上 [所有的递归都能被写成迭代](https://stackoverflow.com/questions/931762/can-every-recursion-be-converted-into-iteration) 。

**普通递归**和**尾递归**这两个概念有点不太好理解，我是不是可以这么想：

- 重要区别有两个：【什么时候**返回**】，【什么时候**计算**】

1. 普通递归，是一直**找到最底，找到了然后才逐层计算并返回结果**，需要记录过程数据📝

> 每次递归调用都会产生一个新的函数实例，每个实例都需要等待其子实例返回结果后才能进行计算并返回自己的结果。

> 这样一来，所有的函数实例都需要在内存中保持活跃状态，直到最底层的实例计算并返回结果，然后逐级传回。

> 因此，需要大量的内存空间来维护这个调用栈。

1. 尾递归，可以**边计算边返回**，不需要记录过程数据📝

> 由于递归调用是函数的最后一步操作，因此在进行递归调用时，不需要保留当前函数实例的状态，可以直接使用新的函数实例替换掉当前实例。

> 无论递归多少次，都只需要一个函数实例的内存空间，大大减少了内存消耗。这也是为什么尾递归对于处理大规模数据或深度递归时具有优势的原因。

当然，这里的`递归调用是函数的最后一步操作`我一开始也有点懵。

如果拿普通递归的代码 ：`return n + res` 和尾递归的代码`return tail_recur(n - 1, res + n)` 单独抽出来理解

- `return n + res` 的 res 是调用递归，整个大函数在调用递归之后还在”等待“，等res回来，要和 n 相加。也就是说，递归调用是函数操作的倒数第二步。
- 而`return tail_recur(n - 1, res + n)` 的 tail_recur(n - 1, res + n)是递归调用，直接就是返回的最后一步操作，对于函数来说，只需要返回这个调用就行，不需要做其他的任何操作，返回之后就结束了，和当前的函数没有任何关系了，可以“释放”了

然后我结合 ChatGPT 给到的类比做了一个优化。

递归这个事情，有点像**拆积木和堆积木**。把 `A 地`的积木拆开，然后**按照顺序**堆到 `B 地`。

1. "递归调用"对应于"拆积木"的动作
2. "返回结果"对应于"堆积木"的动作。
3. 普通递归：把 1、2、3 层的积木先拆开，按顺序摆在桌子上（记住顺序），然后找到最后一层，再开始堆。
4. 尾递归：一边拆积木，一边堆积木，不许要记住顺序，就拆完就堆。

### 2.3  时间复杂度

运行时间可以直观且准确地反映算法的效率。如果我们想准确预估一段代码的运行时间，应该如何操作呢？

1. **确定运行平台**，包括硬件配置、编程语言、系统环境等，这些因素都会影响代码的运行效率。
2. **评估各种计算操作所需的运行时间**，例如加法操作 `+` 需要 1 ns ，乘法操作 `*` 需要 10 ns ，打印操作 `print()` 需要 5 ns 等。
3. **统计代码中所有的计算操作**，并将所有操作的执行时间求和，从而得到运行时间。

但实际上，**统计算法的运行时间既不合理也不现实**。首先，我们不希望将预估时间和运行平台绑定，因为算法需要在各种不同的平台上运行。其次，我们很难获知每种操作的运行时间，这给预估过程带来了极大的难度。

#### 2.3.1  统计时间增长趋势

时间复杂度分析统计的不是算法运行时间，**而是算法运行时间随着数据量变大时的增长趋势**。

```python
# 算法 A 的时间复杂度：常数阶
def algorithm_A(n: int):
    print(0)
# 算法 B 的时间复杂度：线性阶
def algorithm_B(n: int):
    for _ in range(n):
        print(0)
# 算法 C 的时间复杂度：常数阶
def algorithm_C(n: int):
    for _ in range(1000000):
        print(0)
```

图 2-7 展示了以上三个算法函数的时间复杂度。

- 算法 `A` 只有 个打印操作，算法运行时间不随着 增大而增长。我们称此算法的时间复杂度为“常数阶”。
- 算法 `B` 中的打印操作需要循环 次，算法运行时间随着 增大呈线性增长。此算法的时间复杂度被称为“线性阶”。
- 算法 `C` 中的打印操作需要循环 次，虽然运行时间很长，但它与输入数据大小 无关。因此 `C` 的时间复杂度和 `A` 相同，仍为“常数阶”。

[![算法 A、B 和 C 的时间增长趋势](https://www.hello-algo.com/chapter_computational_complexity/time_complexity.assets/time_complexity_simple_example.png)](https://www.hello-algo.com/chapter_computational_complexity/time_complexity.assets/time_complexity_simple_example.png)

图 2-7  算法 A、B 和 C 的时间增长趋势

相较于直接统计算法的运行时间，时间复杂度分析有哪些特点呢？

- **时间复杂度能够有效评估算法效率**。例如，算法 `B` 的运行时间呈线性增长，在n>1时比算法 `A` 更慢，在n>1000000时比算法 `C` 更慢。事实上，只要输入数据大小n足够大，复杂度为“常数阶”的算法一定优于“线性阶”的算法，这正是时间增长趋势的含义。
- **时间复杂度的推算方法更简便**。显然，运行平台和计算操作类型都与算法运行时间的增长趋势无关。因此在时间复杂度分析中，我们可以简单地将所有计算操作的执行时间视为相同的“单位时间”，从而将“计算操作运行时间统计”简化为“计算操作数量统计”，这样一来估算难度就大大降低了。
- **时间复杂度也存在一定的局限性**。例如，尽管算法 `A` 和 `C` 的时间复杂度相同，但实际运行时间差别很大。同样，尽管算法 `B` 的时间复杂度比 `C` 高，但在输入数据大小n较小时，算法 `B` 明显优于算法 `C` 。对于此类情况，我们时常难以仅凭时间复杂度判断算法效率的高低。当然，尽管存在上述问题，复杂度分析仍然是评判算法效率最有效且常用的方法。

#### 2.3.2  函数渐近上界

```python
def algorithm(n: int):
    a = 1      # +1
    a = a + 1  # +1
    a = a * 2  # +1
    # 循环 n 次
    for i in range(n):  # +1
        print(0)        # +1
```

设算法的操作数量是一个关于输入数据大小n的函数，记为T(n)，则以上函数的操作数量为：T(n)=3+2n


T(n)是一次函数，说明其运行时间的增长趋势是线性的，因此它的时间复杂度是线性阶。

我们将线性阶的时间复杂度记为O(n)，这个数学符号称为大O记号（big-O notation），表示函数T(n)的渐近上界（asymptotic upper bound）。

时间复杂度分析本质上是计算“操作数量T(n)”的渐近上界，它具有明确的数学定义。

如图 2-8 所示，计算渐近上界就是寻找一个函数f(n)，使得当n趋向于无穷大时，T(n)和f(n)处于相同的增长级别，仅相差一个常数系数c。

[![函数的渐近上界](https://www.hello-algo.com/chapter_computational_complexity/time_complexity.assets/asymptotic_upper_bound.png)](https://www.hello-algo.com/chapter_computational_complexity/time_complexity.assets/asymptotic_upper_bound.png)

图 2-8  函数的渐近上界

#### 2.3.3  推算方法

1. 第一步：统计操作数量(其实就是看最深层循环次数或递归次数)

   针对代码，逐行从上到下计算即可。然而，由于上述c*f(n)中的常数系数c可以取任意大小，**因此操作数量T(n)中的各种系数、常数项都可以忽略**。根据此原则，可以总结出以下计数简化技巧。

   1. **忽略T(n)中的常数**。因为它们都与n无关，所以对时间复杂度不产生影响。
   2. **省略所有系数**。例如，循环2n次、5n+1次等，都可以简化记为n次，因为n前面的系数对时间复杂度没有影响。
   3. **循环嵌套时使用乘法**。总操作数量等于外层循环和内层循环操作数量之积，每一层循环依然可以分别套用第 `1.` 点和第 `2.` 点的技巧。

2. 第二步：判断渐近上界

   **时间复杂度由T(n)中最高阶的项来决定**。这是因为在n趋于无穷大时，最高阶的项将发挥主导作用，其他项的影响都可以忽略。

#### 2.3.4  常见类型

设输入数据大小为n，常见的时间复杂度类型如图 2-9 所示（按照从低到高的顺序排列）。

O(1)<O($log n$)<O(n)<O($nlog n$)<O($n^2$)<O($2^n$)<O(n!)

常数阶<对数阶<线性阶<线性对数阶<平方阶<指数阶<阶乘阶

![常见的时间复杂度类型](C:\Users\zdz1411\AppData\Roaming\Typora\typora-user-images\image-20250715200425838.png)

图 2-9  常见的时间复杂度类型

1. 常数阶O(1)

   常数阶的操作数量与输入数据大小 无关，即不随着 的变化而变化。

2.  线性阶O(n)

    线性阶的操作数量相对于输入数据大小 以线性级别增长。线性阶通常出现在单层循环中：

3.  平方阶O($n^2$)

    平方阶的操作数量相对于输入数据大小n以平方级别增长。平方阶通常出现在嵌套循环中，外层循环和内层循环的时间复杂度都为O(n)，因此总体的时间复杂度为O($n^2$)

4.  指数阶O($2^n$)

    生物学的“细胞分裂”是指数阶增长的典型例子：初始状态为1个细胞，分裂一轮后变为2个，分裂两轮后变为4个，以此类推，分裂n轮后有$2^n$个细胞。

    图 2-11 和以下代码模拟了细胞分裂的过程，时间复杂度为O($2^n$) 。请注意，输入n表示分裂轮数，返回值 `count` 表示总分裂次数。

    ```python
    def exponential(n: int) -> int:
        """指数阶（循环实现）"""
        count = 0
        base = 1
        # 细胞每轮一分为二，形成数列 1, 2, 4, 8, ..., 2^(n-1)
        for _ in range(n):
            for _ in range(base):
                count += 1
            base *= 2
        # count = 1 + 2 + 4 + 8 + .. + 2^(n-1) = 2^n - 1
        return count
    ```

    [![指数阶的时间复杂度](https://www.hello-algo.com/chapter_computational_complexity/time_complexity.assets/time_complexity_exponential.png)](https://www.hello-algo.com/chapter_computational_complexity/time_complexity.assets/time_complexity_exponential.png)

    图 2-11  指数阶的时间复杂度

    在实际算法中，指数阶常出现于递归函数中。例如在以下代码中，其递归地一分为二，经过 次分裂后停止：

    ```python
    def exp_recur(n: int) -> int:
        """指数阶（递归实现）"""
        if n == 1:
            return 1
        return exp_recur(n - 1) + exp_recur(n - 1) + 1
    ```

    指数阶增长非常迅速，在穷举法（暴力搜索、回溯等）中比较常见。对于数据规模较大的问题，指数阶是不可接受的，通常需要使用动态规划或贪心算法等来解决。

5.  对数阶O($log n$)

    与指数阶相反，对数阶反映了“每轮缩减到一半”的情况。设输入数据大小为n，由于每轮缩减到一半，因此循环次数是$\log_2 n$，即$2^n$的反函数。

    图 2-12 和以下代码模拟了“每轮缩减到一半”的过程，时间复杂度为O($log_2 n$)，简记为O($log_ n$)：

    ```python
    def logarithmic(n: int) -> int:
        """对数阶（循环实现）"""
        count = 0
        while n > 1:
            n = n / 2
            count += 1
        return count
    ```

    ![对数阶的时间复杂度](C:\Users\zdz1411\AppData\Roaming\Typora\typora-user-images\image-20250714165738224.png)

    图 2-12  对数阶的时间复杂度

    与指数阶类似，对数阶也常出现于递归函数中。以下代码形成了一棵高度为$log_2 n$的递归树

    ```python
    def log_recur(n: int) -> int:
        """对数阶（递归实现）"""
        if n <= 1:
            return 0
        return log_recur(n / 2) + 1
    ```

    对数阶常出现于基于分治策略的算法中，体现了“一分为多”和“化繁为简”的算法思想。它增长缓慢，是仅次于常数阶的理想的时间复杂度。

    ![image-20250714165946651](C:\Users\zdz1411\AppData\Roaming\Typora\typora-user-images\image-20250714165946651.png)

6.  线性对数阶O($nlog n$)

    线性对数阶常出现于嵌套循环中，两层循环的时间复杂度分别为O($log n$)和O(n)。

    ```python
    def linear_log_recur(n: int) -> int:
        """线性对数阶"""
        if n <= 1:
            return 1
        # 一分为二，子问题的规模减小一半
        count = linear_log_recur(n // 2) + linear_log_recur(n // 2)
        # 当前子问题包含 n 个操作
        for _ in range(n):
            count += 1
        return count
    ```

    图 2-13 展示了线性对数阶的生成方式。二叉树的每一层的操作总数都为n，树共有$log_2 n+1$层，因此时间复杂度为O($nlog n$)。

    [![线性对数阶的时间复杂度](https://www.hello-algo.com/chapter_computational_complexity/time_complexity.assets/time_complexity_logarithmic_linear.png)](https://www.hello-algo.com/chapter_computational_complexity/time_complexity.assets/time_complexity_logarithmic_linear.png)

    图 2-13  线性对数阶的时间复杂度

    主流排序算法的时间复杂度通常为O($nlog n$)，例如快速排序、归并排序、堆排序等。

7.  阶乘阶O(n!)

    阶乘阶对应数学上的“全排列”问题。给定n个互不重复的元素，求其所有可能的排列方案，方案数量为：
    $$
    n!=n*(n-1)*(n-2)*···*2*1
    $$
    阶乘通常使用递归实现。如图 2-14 和以下代码所示，第一层分裂出n个，第二层分裂出n-1个，以此类推，直至第n层时停止分裂

    ```python
    def factorial_recur(n: int) -> int:
        """阶乘阶（递归实现）"""
        if n == 0:
            return 1
        count = 0
        # 从 1 个分裂出 n 个
        for _ in range(n):
            count += factorial_recur(n - 1)
        return count
    ```

    [![阶乘阶的时间复杂度](https://www.hello-algo.com/chapter_computational_complexity/time_complexity.assets/time_complexity_factorial.png)](https://www.hello-algo.com/chapter_computational_complexity/time_complexity.assets/time_complexity_factorial.png)

    图 2-14  阶乘阶的时间复杂度

    请注意，因为当$n>=4$时恒有$n!>2^n$ ，所以阶乘阶比指数阶增长得更快，在n较大时也是不可接受的。

#### 2.3.5  最差、最佳、平均时间复杂度

“最差时间复杂度”对应函数渐近上界，使用大O记号表示。相应地，“最佳时间复杂度”对应函数渐近下界，用Ω记号表示：

值得说明的是，我们在实际中很少使用最佳时间复杂度，因为通常只有在很小概率下才能达到，可能会带来一定的误导性。**而最差时间复杂度更为实用，因为它给出了一个效率安全值**，让我们可以放心地使用算法。

从上述示例可以看出，最差时间复杂度和最佳时间复杂度只出现于“特殊的数据分布”，这些情况的出现概率可能很小，并不能真实地反映算法运行效率。相比之下，**平均时间复杂度可以体现算法在随机输入数据下的运行效率**，用Θ记号来表示。

对于部分算法，我们可以简单地推算出随机数据分布下的平均情况。比如上述示例，由于输入数组是被打乱的，因此元素1出现在任意索引的概率都是相等的，那么算法的平均循环次数就是数组长度的一半n/2，平均时间复杂度为Θ(n/2)=Θ(n) 。

但对于较为复杂的算法，计算平均时间复杂度往往比较困难，因为很难分析出在数据分布下的整体数学期望。在这种情况下，我们通常使用最差时间复杂度作为算法效率的评判标准。

![为什么很少看到Θ符号？](C:\Users\zdz1411\AppData\Roaming\Typora\typora-user-images\image-20250714171708044.png)

### 2.4  空间复杂度

空间复杂度（space complexity）用于衡量算法占用内存空间随着数据量变大时的增长趋势。这个概念与时间复杂度非常类似，只需将“运行时间”替换为“占用内存空间”。

#### 2.4.1  算法相关空间

算法在运行过程中使用的内存空间主要包括以下几种。

- **输入空间**：用于存储算法的输入数据。
- **暂存空间**：用于存储算法在运行过程中的变量、对象、函数上下文等数据。
- **输出空间**：用于存储算法的输出数据。

一般情况下，空间复杂度的统计范围是“暂存空间”加上“输出空间”。

暂存空间可以进一步划分为三个部分。

- **暂存数据**：用于保存算法运行过程中的各种常量、变量、对象等。
- **栈帧空间**：用于保存调用函数的上下文数据。系统在每次调用函数时都会在栈顶部创建一个栈帧，函数返回后，栈帧空间会被释放。
- **指令空间**：用于保存编译后的程序指令，在实际统计中通常忽略不计。

在分析一段程序的空间复杂度时，**我们通常统计暂存数据、栈帧空间和输出数据三部分**，如图 2-15 所示。

[![算法使用的相关空间](https://www.hello-algo.com/chapter_computational_complexity/space_complexity.assets/space_types.png)](https://www.hello-algo.com/chapter_computational_complexity/space_complexity.assets/space_types.png)

图 2-15  算法使用的相关空间

#### 2.4.2  推算方法

空间复杂度的推算方法与时间复杂度大致相同，只需将统计对象从“操作数量”转为“使用空间大小”。

而与时间复杂度不同的是，**我们通常只关注最差空间复杂度**。这是因为内存空间是一项硬性要求，我们必须确保在所有输入数据下都有足够的内存空间预留。

观察以下代码，最差空间复杂度中的“最差”有两层含义。

1. **以最差输入数据为准**：当n<10时，空间复杂度为O(1)；但当n>10时，初始化的数组 `nums` 占用O(n)空间，因此最差空间复杂度为O(n)。
2. **以算法运行中的峰值内存为准**：例如，程序在执行最后一行之前，占用O(1)空间；当初始化数组 `nums` 时，程序占用O(n)空间，因此最差空间复杂度为O(n)。

```python
def algorithm(n: int):
    a = 0               # O(1)
    b = [0] * 10000     # O(1)
    if n > 10:
        nums = [0] * n  # O(n)
```

**在递归函数中，需要注意统计栈帧空间**。观察以下代码：

```python
def function() -> int:
    # 执行某些操作
    return 0

def loop(n: int):
    """循环的空间复杂度为 O(1)"""
    for _ in range(n):
        function()

def recur(n: int):
    """递归的空间复杂度为 O(n)"""
    if n == 1:
        return
    return recur(n - 1)
```

函数 `loop()` 和 `recur()` 的时间复杂度都为O(n)，但空间复杂度不同。

- 函数 `loop()` 在循环中调用了n次 `function()` ，每轮中的 `function()` 都返回并释放了栈帧空间，因此空间复杂度仍为O(1)。
- 递归函数 `recur()` 在运行过程中会同时存在n个未返回的 `recur()` ，从而占用O(n)的栈帧空间。

#### 2.4.3  常见类型

设输入数据大小为n，图 2-16 展示了常见的空间复杂度类型（从低到高排列）。

O(1)<O($log_2 n$)<O(n)<O($n^2$)<O($2^n$)

常数阶 < 对数阶 < 线性阶 < 平方阶 < 指数阶

![常见的空间复杂度类型](https://www.hello-algo.com/chapter_computational_complexity/space_complexity.assets/space_complexity_common_types.png)

图 2-16  常见的空间复杂度类型

1. 常数阶O(1)

   常数阶常见于数量与输入数据大小n无关的常量、变量、对象。

   需要注意的是，在循环中初始化变量或调用函数而占用的内存，在进入下一循环后就会被释放，因此不会累积占用空间，空间复杂度仍为O(1)： 

   ```python
   def function() -> int:
       """函数"""
       # 执行某些操作
       return 0
   
   def constant(n: int):
       """常数阶"""
       # 常量、变量、对象占用 O(1) 空间
       a = 0
       nums = [0] * 10000
       node = ListNode(0)
       # 循环中的变量占用 O(1) 空间
       for _ in range(n):
           c = 0
       # 循环中的函数占用 O(1) 空间
       for _ in range(n):
           function()
   ```

2. 线性阶O(n)

   线性阶常见于元素数量与n成正比的数组、链表、栈、队列等：

   ```python
   def linear(n: int):
       """线性阶"""
       # 长度为 n 的列表占用 O(n) 空间
       nums = [0] * n
       # 长度为 n 的哈希表占用 O(n) 空间
       hmap = dict[int, str]()
       for i in range(n):
           hmap[i] = str(i)
   ```

   如图 2-17 所示，此函数的递归深度为n，即同时存在n个未返回的 `linear_recur()` 函数，使用O(n)大小的栈帧空间：

   ```python
   def linear_recur(n: int):
       """线性阶（递归实现）"""
       print("递归 n =", n)
       if n == 1:
           return
       linear_recur(n - 1)
   ```

   ![递归函数产生的线性阶空间复杂度](https://www.hello-algo.com/chapter_computational_complexity/space_complexity.assets/space_complexity_recursive_linear.png)

   图 2-17  递归函数产生的线性阶空间复杂度

3. 平方阶O($n^2$)

   ```python
   def quadratic(n: int):
       """平方阶"""
       # 二维列表占用 O(n^2) 空间
       num_matrix = [[0] * n for _ in range(n)]
   ```

   如图 2-18 所示，该函数的递归深度为n，在每个递归函数中都初始化了一个数组，长度分别为n、n-1、...、2、1 ，平均长度为n/2，因此总体占用O($n^2$)空间：

   ```python
   def quadratic_recur(n: int) -> int:
       """平方阶（递归实现）"""
       if n <= 0:
           return 0
       # 数组 nums 长度为 n, n-1, ..., 2, 1
       nums = [0] * n
       return quadratic_recur(n - 1)
   ```

   [![递归函数产生的平方阶空间复杂度](https://www.hello-algo.com/chapter_computational_complexity/space_complexity.assets/space_complexity_recursive_quadratic.png)](https://www.hello-algo.com/chapter_computational_complexity/space_complexity.assets/space_complexity_recursive_quadratic.png)

   图 2-18  递归函数产生的平方阶空间复杂度

4. 指数阶O($2^n$)

   指数阶常见于二叉树。观察图 2-19 ，层数为n的“满二叉树”的节点数量为$2^n-1$，占用O($2^n$)空间：

   ```python
   def build_tree(n: int) -> TreeNode | None:
       """指数阶（建立满二叉树）"""
       if n == 0:
           return None
       root = TreeNode(0)
       root.left = build_tree(n - 1)
       root.right = build_tree(n - 1)
       return root
   ```

   [![满二叉树产生的指数阶空间复杂度](https://www.hello-algo.com/chapter_computational_complexity/space_complexity.assets/space_complexity_exponential.png)](https://www.hello-algo.com/chapter_computational_complexity/space_complexity.assets/space_complexity_exponential.png)

   图 2-19  满二叉树产生的指数阶空间复杂度

5. 对数阶O($log n$)

   对数阶常见于分治算法。例如归并排序，输入长度为n的数组，每轮递归将数组从中点处划分为两半，形成高度为$log n$的递归树，使用O($log n$)栈帧空间。

   再例如将数字转化为字符串，输入一个正整数n，它的位数为$⌊log_{10}n⌋+1$，即对应字符串长度为$⌊log_{10}n⌋+1$，因此空间复杂度为 O($log_{10}n+1$)=O($log n$)。

#### 2.4.4  权衡时间与空间

理想情况下，我们希望算法的时间复杂度和空间复杂度都能达到最优。然而在实际情况中，同时优化时间复杂度和空间复杂度通常非常困难。

**降低时间复杂度通常需要以提升空间复杂度为代价，反之亦然**。我们将牺牲内存空间来提升算法运行速度的思路称为“以空间换时间”；反之，则称为“以时间换空间”。

选择哪种思路取决于我们更看重哪个方面。在大多数情况下，时间比空间更宝贵，因此“以空间换时间”通常是更常用的策略。当然，在数据量很大的情况下，控制空间复杂度也非常重要。

### 2.5  Q&A

**Q**：尾递归的空间复杂度是O(1)吗？

理论上，尾递归函数的空间复杂度可以优化至O(1)。不过绝大多数编程语言（例如 Java、Python、C++、Go、C# 等）不支持自动优化尾递归，因此通常认为空间复杂度是O(n)。

**Q**：函数和方法这两个术语的区别是什么？

函数（function）可以被独立执行，所有参数都以显式传递。方法（method）与一个对象关联，被隐式传递给调用它的对象，能够对类的实例中包含的数据进行操作。

下面以几种常见的编程语言为例来说明。

- C 语言是过程式编程语言，没有面向对象的概念，所以只有函数。但我们可以通过创建结构体（struct）来模拟面向对象编程，与结构体相关联的函数就相当于其他编程语言中的方法。
- Java 和 C# 是面向对象的编程语言，代码块（方法）通常作为某个类的一部分。静态方法的行为类似于函数，因为它被绑定在类上，不能访问特定的实例变量。
- C++ 和 Python 既支持过程式编程（函数），也支持面向对象编程（方法）。

**Q**：图解“常见的空间复杂度类型”反映的是否是占用空间的绝对大小？

不是，该图展示的是空间复杂度，其反映的是增长趋势，而不是占用空间的绝对大小。

假设取n=8，你可能会发现每条曲线的值与函数对应不上。这是因为每条曲线都包含一个常数项，用于将取值范围压缩到一个视觉舒适的范围内。

在实际中，因为我们通常不知道每个方法的“常数项”复杂度是多少，所以一般无法仅凭复杂度来选择n=8之下的最优解法。但对于n=8^5就很好选了，这时增长趋势已经占主导了。

## 第 3 章  数据结构

![数据结构](https://www.hello-algo.com/assets/covers/chapter_data_structure.jpg)

### 3.1  数据结构分类

常见的数据结构包括数组、链表、栈、队列、哈希表、树、堆、图，它们可以从“逻辑结构”和“物理结构”两个维度进行分类。

#### 3.1.1  逻辑结构：线性与非线性

**逻辑结构揭示了数据元素之间的逻辑关系**。在数组和链表中，数据按照一定顺序排列，体现了数据之间的线性关系；而在树中，数据从顶部向下按层次排列，表现出“祖先”与“后代”之间的派生关系；图则由节点和边构成，反映了复杂的网络关系。

如图 3-1 所示，逻辑结构可分为“线性”和“非线性”两大类。线性结构比较直观，指数据在逻辑关系上呈线性排列；非线性结构则相反，呈非线性排列。

- **线性数据结构**：数组、链表、栈、队列、哈希表，元素之间是一对一的顺序关系。
- **非线性数据结构**：树、堆、图、哈希表。

![线性数据结构与非线性数据结构](https://www.hello-algo.com/chapter_data_structure/classification_of_data_structure.assets/classification_logic_structure.png)

图 3-1  线性数据结构与非线性数据结构

#### 3.1.2  物理结构：连续与分散

**当算法程序运行时，正在处理的数据主要存储在内存中**。图 3-2 展示了一个计算机内存条，其中每个黑色方块都包含一块内存空间。我们可以将内存想象成一个巨大的 Excel 表格，其中每个单元格都可以存储一定大小的数据。

**系统通过内存地址来访问目标位置的数据**。如图 3-2 所示，计算机根据特定规则为表格中的每个单元格分配编号，确保每个内存空间都有唯一的内存地址。有了这些地址，程序便可以访问内存中的数据。

[![内存条、内存空间、内存地址](https://www.hello-algo.com/chapter_data_structure/classification_of_data_structure.assets/computer_memory_location.png)](https://www.hello-algo.com/chapter_data_structure/classification_of_data_structure.assets/computer_memory_location.png)

图 3-2  内存条、内存空间、内存地址

> 值得说明的是，将内存比作 Excel 表格是一个简化的类比，实际内存的工作机制比较复杂，涉及地址空间、内存管理、缓存机制、虚拟内存和物理内存等概念。

内存是所有程序的共享资源，当某块内存被某个程序占用时，则通常无法被其他程序同时使用了。**因此在数据结构与算法的设计中，内存资源是一个重要的考虑因素**。比如，算法所占用的内存峰值不应超过系统剩余空闲内存；如果缺少连续大块的内存空间，那么所选用的数据结构必须能够存储在分散的内存空间内。

如图 3-3 所示，**物理结构反映了数据在计算机内存中的存储方式**，可分为连续空间存储（数组）和分散空间存储（链表）。物理结构从底层决定了数据的访问、更新、增删等操作方法，两种物理结构在时间效率和空间效率方面呈现出互补的特点。

[![连续空间存储与分散空间存储](https://www.hello-algo.com/chapter_data_structure/classification_of_data_structure.assets/classification_phisical_structure.png)](https://www.hello-algo.com/chapter_data_structure/classification_of_data_structure.assets/classification_phisical_structure.png)

图 3-3  连续空间存储与分散空间存储

值得说明的是，**所有数据结构都是基于数组、链表或二者的组合实现的**。例如，栈和队列既可以使用数组实现，也可以使用链表实现；而哈希表的实现可能同时包含数组和链表。

- **基于数组可实现**：栈、队列、哈希表、树、堆、图、矩阵、张量（维度 的数组）等。
- **基于链表可实现**：栈、队列、哈希表、树、堆、图等。

链表在初始化后，仍可以在程序运行过程中对其长度进行调整，因此也称“动态数据结构”。数组在初始化后长度不可变，因此也称“静态数据结构”。值得注意的是，数组可通过重新分配内存实现长度变化，从而具备一定的“动态性”。

#### Comment

假设我们初始化一个很大的数组。在虚拟内存中，数组的内存空间是连续的。这样我们才能通过首元素地址加偏移量的方式来访问数组中的任何元素。

然而，在物理内存中，数组可能并不连续。虚拟内存到物理内存的映射涉及到分页（paging）技术。操作系统通过分页系统，将虚拟内存和物理内存都划分为大小相等的页。每个虚拟页都可以单独映射到物理内存中的任意一页。所以，一个大的数组（占据多个虚拟内存页）在物理内存中可能是分散的，每个虚拟页在物理内存中的位置可能不连续。

值得强调的是，虽然物理内存中的连续性可能会对性能有影响，但在实际应用中，这通常并不是主要的性能瓶颈。现代的计算机系统已经采用了许多优化技术，例如缓存、预取等，以尽可能减小这种影响。

### 3.2  基本数据类型

当谈及计算机中的数据时，我们会想到文本、图片、视频、语音、3D 模型等各种形式。尽管这些数据的组织形式各异，但它们都由各种基本数据类型构成。

**基本数据类型是 CPU 可以直接进行运算的类型**，在算法中直接被使用，主要包括以下几种。

- 整数类型 `byte`、`short`、`int`、`long` 。
- 浮点数类型 `float`、`double` ，用于表示小数。
- 字符类型 `char` ，用于表示各种语言的字母、标点符号甚至表情符号等。
- 布尔类型 `bool` ，用于表示“是”与“否”判断。

**基本数据类型以二进制的形式存储在计算机中**。一个二进制位即为1比特。在绝大多数现代操作系统中，1字节（byte）由8比特（bit）组成。

基本数据类型的取值范围取决于其占用的空间大小。下面以 Java 为例。

- 整数类型 `byte` 占用1字节 =8比特 ，可以表示$2^8$个数字。
- 整数类型 `int` 占用4字节 =32比特 ，可以表示$2^{32}$个数字。

表 3-1 列举了 Java 中各种基本数据类型的占用空间、取值范围和默认值。此表格无须死记硬背，大致理解即可，需要时可以通过查表来回忆。

表 3-1  基本数据类型的占用空间和取值范围

![基本数据类型的占用空间和取值范围](C:\Users\zdz1411\AppData\Roaming\Typora\typora-user-images\image-20250717184144152.png)

请注意，表 3-1 针对的是 Java 的基本数据类型的情况。每种编程语言都有各自的数据类型定义，它们的占用空间、取值范围和默认值可能会有所不同。

- 在 Python 中，整数类型 `int` 可以是任意大小，只受限于可用内存；浮点数 `float` 是双精度 64 位；没有 `char` 类型，单个字符实际上是长度为 1 的字符串 `str` 。
- C 和 C++ 未明确规定基本数据类型的大小，而因实现和平台各异。表 3-1 遵循 LP64 [数据模型](https://en.cppreference.com/w/cpp/language/types#Properties)，其用于包括 Linux 和 macOS 在内的 Unix 64 位操作系统。
- 字符 `char` 的大小在 C 和 C++ 中为 1 字节，在大多数编程语言中取决于特定的字符编码方法，详见“字符编码”章节。
- 即使表示布尔量仅需 1 位（0或1），它在内存中通常也存储为 1 字节。这是因为现代计算机 CPU 通常将 1 字节作为最小寻址内存单元。

那么，基本数据类型与数据结构之间有什么联系呢？我们知道，数据结构是在计算机中组织与存储数据的方式。这句话的主语是“结构”而非“数据”。

如果想表示“一排数字”，我们自然会想到使用数组。这是因为数组的线性结构可以表示数字的相邻关系和顺序关系，但至于存储的内容是整数 `int`、小数 `float` 还是字符 `char` ，则与“数据结构”无关。

换句话说，**基本数据类型提供了数据的“内容类型”，而数据结构提供了数据的“组织方式”**。例如以下代码，我们用相同的数据结构（数组）来存储与表示不同的基本数据类型，包括 `int`、`float`、`char`、`bool` 等。

```python
# 使用多种基本数据类型来初始化数组
numbers: list[int] = [0] * 5
decimals: list[float] = [0.0] * 5
# Python 的字符实际上是长度为 1 的字符串
characters: list[str] = ['0'] * 5
bools: list[bool] = [False] * 5
# Python 的列表可以自由存储各种基本数据类型和对象引用
data = [0, 0.0, 'a', False, ListNode(0)]
```

### 3.3  数字编码 *

#### 3.3.1  原码、反码和补码

在上一节的表格中我们发现，所有整数类型能够表示的负数都比正数多一个，例如 `byte` 的取值范围是[-128,127]。这个现象比较反直觉，它的内在原因涉及原码、反码、补码的相关知识。

首先需要指出，**数字是以“补码”的形式存储在计算机中的**。在分析这样做的原因之前，首先给出三者的定义。

- **原码**：我们将数字的二进制表示的最高位视为符号位，其中0表示正数，1表示负数，其余位表示数字的值。
- **反码**：正数的反码与其原码相同，负数的反码是对其原码除符号位外的所有位取反。
- **补码**：正数的补码与其原码相同，负数的补码是在其反码的基础上加1。

图 3-4 展示了原码、反码和补码之间的转换方法。

负数原码转补码，从右往左找到第一个1，包含1在内的往右都不变，左边全部取反（符号位不变）

![原码、反码与补码之间的相互转换](https://www.hello-algo.com/chapter_data_structure/number_encoding.assets/1s_2s_complement.png)

图 3-4  原码、反码与补码之间的相互转换

原码（sign-magnitude）虽然最直观，但存在一些局限性。一方面，**负数的原码不能直接用于运算**。例如在原码下计算1+(-2)，得到的结果是-3，这显然是不对的。

1+(-2)

->0000  0001+1000  0010

=1000 0011

->-3

为了解决此问题，计算机引入了反码（1's complement）。如果我们先将原码转换为反码，并在反码下计算1+(-2)，最后将结果从反码转换回原码，则可得到正确结果-1。

1+(-2)

->0000  0001(原码)+1000  0010(原码)

=0000  0001(反码)+1111  1101(反码)

=1111  1110(反码)

=1000  0001(原码)

->-1

另一方面，**数字零的原码有-0和+0两种表示方式**。这意味着数字零对应两个不同的二进制编码，这可能会带来歧义。比如在条件判断中，如果没有区分正零和负零，则可能会导致判断结果出错。而如果我们想处理正零和负零歧义，则需要引入额外的判断操作，这可能会降低计算机的运算效率。

+0->0000  0000

-0->1000  0000

与原码一样，反码也存在正负零歧义问题，因此计算机进一步引入了补码（2's complement）。我们先来观察一下负零的原码、反码、补码的转换过程：

-0->1000  0000(原码)

=1111  1111(反码)

=1  0000  0000(补码)

在负零的反码基础上加1会产生进位，但 `byte` 类型的长度只有 8 位，因此溢出到第 9 位的1会被舍弃。也就是说，**负零的补码为0000  0000，与正零的补码相同**。这意味着在补码表示中只存在一个零，正负零歧义从而得到解决。

还剩最后一个疑惑：`byte` 类型的取值范围是[-128,+127] ，多出来的一个负数-128是如何得到的呢？我们注意到，区间\[-127,+127]内的所有整数都有对应的原码、反码和补码，并且原码和补码之间可以互相转换。

然而，**补码1000  0000是一个例外，它并没有对应的原码**。根据转换方法，我们得到该补码的原码为0000  0000。这显然是矛盾的，因为该原码表示数字 ，它的补码应该是自身。计算机规定这个特殊的补码1000 0000代表-128。实际上，(-1)+(-127)在补码下的计算结果就是-128。

(-127)+(-1)

->1111  1111(原码)+1000  0001(原码)

=1000  0000(反码)+1111  1110(反码)

=1000  0001(补码)+1111  1111(补码)

=1000  0000(补码)

->-128

你可能已经发现了，上述所有计算都是加法运算。这暗示着一个重要事实：**计算机内部的硬件电路主要是基于加法运算设计的**。这是因为加法运算相对于其他运算（比如乘法、除法和减法）来说，硬件实现起来更简单，更容易进行并行化处理，运算速度更快。

请注意，这并不意味着计算机只能做加法。**通过将加法与一些基本逻辑运算结合，计算机能够实现各种其他的数学运算**。例如，计算减法a-b可以转换为a+(-b)计算加法 ；计算乘法和除法可以转换为计算多次加法或减法。

现在我们可以总结出计算机使用补码的原因：基于补码表示，计算机可以用同样的电路和操作来处理正数和负数的加法，不需要设计特殊的硬件电路来处理减法，并且无须特别处理正负零的歧义问题。这大大简化了硬件设计，提高了运算效率。

补码的设计非常精妙，因篇幅关系我们就先介绍到这里，建议有兴趣的读者进一步深入了解。

#### 3.3.2  浮点数编码

细心的你可能会发现：`int` 和 `float` 长度相同，都是 4 字节 ，但为什么 `float` 的取值范围远大于 `int` ？这非常反直觉，因为按理说 `float` 需要表示小数，取值范围应该变小才对。

实际上，**这是因为浮点数 `float` 采用了不同的表示方式**。记一个 32 比特长度的二进制数为：
$$
b_{31}b_{30}b_{29}...b_{2}b_{1}b_{0}
$$
据 IEEE 754 标准，32-bit 长度的 `float` 由以下三个部分构成。

- 符号位S：占 1 位 ，对应$b_{31}$。
- 指数位E：占 8 位 ，对应$b_{30}b_{29}...b_{23}$。
- 分数位N：占 23 位 ，对应$b_{22}b_{21}...b_{0}$ 。

二进制数 `float` 对应值的计算方法为：
$$
val=(-1)^{b_{31}}×2^{({{b_{30}b_{29}...b_{23})}_2-127}}×(1.b_{22}b_{21}...b_{0})_2
$$
转化到十进制下的计算公式为：
$$
val=(-1)^S×2^{E-127}×(1+N)
$$
其中各项的取值范围为：

![image-20250717200216151](C:\Users\zdz1411\AppData\Roaming\Typora\typora-user-images\image-20250717200216151.png)

![IEEE 754 标准下的 float 的计算示例](https://www.hello-algo.com/chapter_data_structure/number_encoding.assets/ieee_754_float.png)

图 3-5  IEEE 754 标准下的 float 的计算示例

现在我们可以回答最初的问题：**`float` 的表示方式包含指数位，导致其取值范围远大于 `int`** 。根据以上计算，`float` 可表示的最大正数为$2^{254-127}×(2-2^{-23})≈3.4×10^{38}$，切换符号位便可得到最小负数。

**尽管浮点数 `float` 扩展了取值范围，但其副作用是牺牲了精度**。整数类型 `int` 将全部 32 比特用于表示数字，数字是均匀分布的；而由于指数位的存在，浮点数 `float` 的数值越大，相邻两个数字之间的差值就会趋向越大。

![image-20250717200740452](C:\Users\zdz1411\AppData\Roaming\Typora\typora-user-images\image-20250717200740452.png)

#### Comment

![image-20250717203552149](C:\Users\zdz1411\AppData\Roaming\Typora\typora-user-images\image-20250717203552149.png)

![image-20250717204002910](C:\Users\zdz1411\AppData\Roaming\Typora\typora-user-images\image-20250717204002910.png)

- 原码：直观但减法复杂，零有冗余；
- 反码：实现减法转加法，但零仍冗余，需处理循环进位；
- 补码：彻底解决零的冗余和进位问题，实现了加减法运算的统一与高效，最终成为计算机中表示有符号整数的通用标准。

**进位循环处理**：反码加法时，若最高位产生进位，需将进位 “循环” 加到结果的最低位（称为 “循环进位”），增加了硬件运算的复杂度。例如 `3 + (-3)`：
`3` 反码 `00000011` + `-3` 反码 `11111100` = `11111111`（进位 1 被循环加到最低位后仍为 `11111111`，即 `-0`），逻辑不够简洁。

### 3.4  字符编码 *

在计算机中，所有数据都是以二进制数的形式存储的，字符 `char` 也不例外。为了表示字符，我们需要建立一套“字符集”，规定每个字符和二进制数之间的一一对应关系。有了字符集之后，计算机就可以通过查表完成二进制数到字符的转换。

#### 3.4.1  ASCII 字符集

ASCII 码是最早出现的字符集，其全称为 American Standard Code for Information Interchange（美国标准信息交换代码）。它使用 7 位二进制数（一个字节的低 7 位）表示一个字符，最多能够表示 128 个不同的字符。如图 3-6 所示，ASCII 码包括英文字母的大小写、数字 0 ~ 9、一些标点符号，以及一些控制字符（如换行符和制表符）。

![ASCII 码](https://www.hello-algo.com/chapter_data_structure/character_encoding.assets/ascii_table.png)

图 3-6  ASCII 码

然而，**ASCII 码仅能够表示英文**。随着计算机的全球化，诞生了一种能够表示更多语言的 EASCII 字符集。它在 ASCII 的 7 位基础上扩展到 8 位，能够表示 256 个不同的字符。

在世界范围内，陆续出现了一批适用于不同地区的 EASCII 字符集。这些字符集的前 128 个字符统一为 ASCII 码，后 128 个字符定义不同，以适应不同语言的需求。

#### 3.4.2  GBK 字符集

后来人们发现，**EASCII 码仍然无法满足许多语言的字符数量要求**。比如汉字有近十万个，光日常使用的就有几千个。中国国家标准总局于 1980 年发布了 GB2312 字符集，其收录了 6763 个汉字，基本满足了汉字的计算机处理需要。

然而，GB2312 无法处理部分罕见字和繁体字。GBK 字符集是在 GB2312 的基础上扩展得到的，它共收录了 21886 个汉字。在 GBK 的编码方案中，ASCII 字符使用一个字节表示，汉字使用两个字节表示。

#### 3.4.3  Unicode 字符集

随着计算机技术的蓬勃发展，字符集与编码标准百花齐放，而这带来了许多问题。一方面，这些字符集一般只定义了特定语言的字符，无法在多语言环境下正常工作。另一方面，同一种语言存在多种字符集标准，如果两台计算机使用的是不同的编码标准，则在信息传递时就会出现乱码。

那个时代的研究人员就在想：**如果推出一个足够完整的字符集，将世界范围内的所有语言和符号都收录其中，不就可以解决跨语言环境和乱码问题了吗**？在这种想法的驱动下，一个大而全的字符集 Unicode 应运而生。

Unicode 的中文名称为“统一码”，理论上能容纳 100 多万个字符。它致力于将全球范围内的字符纳入统一的字符集之中，提供一种通用的字符集来处理和显示各种语言文字，减少因为编码标准不同而产生的乱码问题。

自 1991 年发布以来，Unicode 不断扩充新的语言与字符。截至 2022 年 9 月，Unicode 已经包含 149186 个字符，包括各种语言的字符、符号甚至表情符号等。在庞大的 Unicode 字符集中，常用的字符占用 2 字节，有些生僻的字符占用 3 字节甚至 4 字节。

Unicode 是一种通用字符集，本质上是给每个字符分配一个编号（称为“码点”），**但它并没有规定在计算机中如何存储这些字符码点**。我们不禁会问：当多种长度的 Unicode 码点同时出现在一个文本中时，系统如何解析字符？例如给定一个长度为 2 字节的编码，系统如何确认它是一个 2 字节的字符还是两个 1 字节的字符？

对于以上问题，**一种直接的解决方案是将所有字符存储为等长的编码**。如图 3-7 所示，“Hello”中的每个字符占用 1 字节，“算法”中的每个字符占用 2 字节。我们可以通过高位填 0 将“Hello 算法”中的所有字符都编码为 2 字节长度。这样系统就可以每隔 2 字节解析一个字符，恢复这个短语的内容了。

![Unicode 编码示例](https://www.hello-algo.com/chapter_data_structure/character_encoding.assets/unicode_hello_algo.png)

图 3-7  Unicode 编码示例

然而 ASCII 码已经向我们证明，编码英文只需 1 字节。若采用上述方案，英文文本占用空间的大小将会是 ASCII 编码下的两倍，非常浪费内存空间。因此，我们需要一种更加高效的 Unicode 编码方法。

#### 3.4.4  UTF-8 编码

目前，UTF-8 已成为国际上使用最广泛的 Unicode 编码方法。**它是一种可变长度的编码**，使用 1 到 4 字节来表示一个字符，根据字符的复杂性而变。ASCII 字符只需 1 字节，拉丁字母和希腊字母需要 2 字节，常用的中文字符需要 3 字节，其他的一些生僻字符需要 4 字节。

UTF-8 的编码规则并不复杂，分为以下两种情况。

- 对于长度为 1 字节的字符，将最高位设置为0，其余 7 位设置为 Unicode 码点。值得注意的是，ASCII 字符在 Unicode 字符集中占据了前 128 个码点。也就是说，**UTF-8 编码可以向下兼容 ASCII 码**。这意味着我们可以使用 UTF-8 来解析年代久远的 ASCII 码文本。
- 对于长度为 字节的字符（其中n>1），将首个字节的高n位都设置为1，第n+1位设置为0；从第二个字节开始，将每个字节的高 2 位都设置为10；其余所有位用于填充字符的 Unicode 码点。

图 3-8 展示了“Hello算法”对应的 UTF-8 编码。观察发现，由于最高n位都设置为1，因此系统可以通过读取最高位1的个数来解析出字符的长度为n。

但为什么要将其余所有字节的高 2 位都设置为10呢？实际上，这个10能够起到校验符的作用。假设系统从一个错误的字节开始解析文本，字节头部的10能够帮助系统快速判断出异常。

之所以将10当作校验符，是因为在 UTF-8 编码规则下，不可能有字符的最高两位是10。这个结论可以用反证法来证明：假设一个字符的最高两位是10，说明该字符的长度为1，对应 ASCII 码。而 ASCII 码的最高位应该是0，与假设矛盾。

![UTF-8 编码示例](https://www.hello-algo.com/chapter_data_structure/character_encoding.assets/utf-8_hello_algo.png)

图 3-8  UTF-8 编码示例

除了 UTF-8 之外，常见的编码方式还包括以下两种。

- **UTF-16 编码**：使用 2 或 4 字节来表示一个字符。所有的 ASCII 字符和常用的非英文字符，都用 2 字节表示；少数字符需要用到 4 字节表示。对于 2 字节的字符，UTF-16 编码与 Unicode 码点相等。
- **UTF-32 编码**：每个字符都使用 4 字节。这意味着 UTF-32 比 UTF-8 和 UTF-16 更占用空间，特别是对于 ASCII 字符占比较高的文本。

从存储空间占用的角度看，使用 UTF-8 表示英文字符非常高效，因为它仅需 1 字节；使用 UTF-16 编码某些非英文字符（例如中文）会更加高效，因为它仅需 2 字节，而 UTF-8 可能需要 3 字节。

从兼容性的角度看，UTF-8 的通用性最佳，许多工具和库优先支持 UTF-8 。

#### 3.4.5  编程语言的字符编码

对于以往的大多数编程语言，程序运行中的字符串都采用 UTF-16 或 UTF-32 这类等长编码。在等长编码下，我们可以将字符串看作数组来处理，这种做法具有以下优点。

- **随机访问**：UTF-16 编码的字符串可以很容易地进行随机访问。UTF-8 是一种变长编码，要想找到第i个字符，我们需要从字符串的开始处遍历到第i个字符，这需要O(n)的时间。
- **字符计数**：与随机访问类似，计算 UTF-16 编码的字符串的长度也是O(1)的操作。但是，计算 UTF-8 编码的字符串的长度需要遍历整个字符串。
- **字符串操作**：在 UTF-16 编码的字符串上，很多字符串操作（如分割、连接、插入、删除等）更容易进行。在 UTF-8 编码的字符串上，进行这些操作通常需要额外的计算，以确保不会产生无效的 UTF-8 编码。

实际上，编程语言的字符编码方案设计是一个很有趣的话题，涉及许多因素。

- Java 的 `String` 类型使用 UTF-16 编码，每个字符占用 2 字节。这是因为 Java 语言设计之初，人们认为 16 位足以表示所有可能的字符。然而，这是一个不正确的判断。后来 Unicode 规范扩展到了超过 16 位，所以 Java 中的字符现在可能由一对 16 位的值（称为“代理对”）表示。
- JavaScript 和 TypeScript 的字符串使用 UTF-16 编码的原因与 Java 类似。当 1995 年 Netscape 公司首次推出 JavaScript 语言时，Unicode 还处于发展早期，那时候使用 16 位的编码就足以表示所有的 Unicode 字符了。
- C# 使用 UTF-16 编码，主要是因为 .NET 平台是由 Microsoft 设计的，而 Microsoft 的很多技术（包括 Windows 操作系统）都广泛使用 UTF-16 编码。

由于以上编程语言对字符数量的低估，它们不得不采取“代理对”的方式来表示超过 16 位长度的 Unicode 字符。这是一个不得已为之的无奈之举。一方面，包含代理对的字符串中，一个字符可能占用 2 字节或 4 字节，从而丧失了等长编码的优势。另一方面，处理代理对需要额外增加代码，这提高了编程的复杂性和调试难度。

出于以上原因，部分编程语言提出了一些不同的编码方案。

- Python 中的 `str` 使用 Unicode 编码，并采用一种灵活的字符串表示，存储的字符长度取决于字符串中最大的 Unicode 码点。若字符串中全部是 ASCII 字符，则每个字符占用 1 字节；如果有字符超出了 ASCII 范围，但全部在基本多语言平面（BMP）内，则每个字符占用 2 字节；如果有超出 BMP 的字符，则每个字符占用 4 字节。
- Go 语言的 `string` 类型在内部使用 UTF-8 编码。Go 语言还提供了 `rune` 类型，它用于表示单个 Unicode 码点。
- Rust 语言的 `str` 和 `String` 类型在内部使用 UTF-8 编码。Rust 也提供了 `char` 类型，用于表示单个 Unicode 码点。

需要注意的是，以上讨论的都是字符串在编程语言中的存储方式，**这和字符串如何在文件中存储或在网络中传输是不同的问题**。在文件存储或网络传输中，我们通常会将字符串编码为 UTF-8 格式，以达到最优的兼容性和空间效率。

#### Comment

Python 中的 `str` 字符串直接用 Unicode 表示（请注意区分：读写文件默认为 UTF-8 编码）。CPython 为 str 对象使用了一种灵活的存储策略，字符长度取决于字符串中最大的字符。观察以下测试：

```python
import sys

eng1 = "a"
eng2 = "ab"
eng3 = "abc"

chn1 = "哈"
chn2 = "哈啰"
chn2_eng1 = "哈啰a"

bmp1 = "𨊻"
bmp2 = "𨊻𨋾"
bmp2_eng1 = "𨊻𨋾a"

print("\n英文：") # 英文长度为 1
print(eng1 + ": ", sys.getsizeof(eng1))
print(eng2 + ": ", sys.getsizeof(eng2))
print(eng3 + ": ", sys.getsizeof(eng3))

print("\n中文:")
print(chn1 + ": ", sys.getsizeof(chn1))
print(chn2 + ": ", sys.getsizeof(chn2))
print(chn2_eng1 + ": ", sys.getsizeof(chn2_eng1))

print("\n补充平面:")
print(bmp1 + ": ", sys.getsizeof(bmp1))
print(bmp2 + ": ", sys.getsizeof(bmp2))
print(bmp2_eng1 + ": ", sys.getsizeof(bmp2_eng1))
```

可以得到以下结论：

1. 纯英文字符串的字符长度为 1 ；
2. 中文字符串下，中文和英文长度都为 2 ；
3. 补充平面字符下，英文字符长度为 4 ；

```python
英文：
a:  50
ab:  51
abc:  52

中文:
哈:  76
哈啰:  78
哈啰a:  80

超平面:
𨊻:  80
𨊻𨋾:  84
𨊻𨋾a:  88
```

![image-20250717214907640](C:\Users\zdz1411\AppData\Roaming\Typora\typora-user-images\image-20250717214907640.png)

### 3.5  小结

####  重点回顾

- 数据结构可以从逻辑结构和物理结构两个角度进行分类。逻辑结构描述了数据元素之间的逻辑关系，而物理结构描述了数据在计算机内存中的存储方式。
- 常见的逻辑结构包括线性、树状和网状等。通常我们根据逻辑结构将数据结构分为线性（数组、链表、栈、队列）和非线性（树、图、堆）两种。哈希表的实现可能同时包含线性数据结构和非线性数据结构。
- 当程序运行时，数据被存储在计算机内存中。每个内存空间都拥有对应的内存地址，程序通过这些内存地址访问数据。
- 物理结构主要分为连续空间存储（数组）和分散空间存储（链表）。所有数据结构都是由数组、链表或两者的组合实现的。
- 计算机中的基本数据类型包括整数 `byte`、`short`、`int`、`long` ，浮点数 `float`、`double` ，字符 `char` 和布尔 `bool` 。它们的取值范围取决于占用空间大小和表示方式。
- 原码、反码和补码是在计算机中编码数字的三种方法，它们之间可以相互转换。整数的原码的最高位是符号位，其余位是数字的值。
- 整数在计算机中是以补码的形式存储的。在补码表示下，计算机可以对正数和负数的加法一视同仁，不需要为减法操作单独设计特殊的硬件电路，并且不存在正负零歧义的问题。
- 浮点数的编码由 1 位符号位、8 位指数位和 23 位分数位构成。由于存在指数位，因此浮点数的取值范围远大于整数，代价是牺牲了精度。
- ASCII 码是最早出现的英文字符集，长度为 1 字节，共收录 127 个字符。GBK 字符集是常用的中文字符集，共收录两万多个汉字。Unicode 致力于提供一个完整的字符集标准，收录世界上各种语言的字符，从而解决由于字符编码方法不一致而导致的乱码问题。
- UTF-8 是最受欢迎的 Unicode 编码方法，通用性非常好。它是一种变长的编码方法，具有很好的扩展性，有效提升了存储空间的使用效率。UTF-16 和 UTF-32 是等长的编码方法。在编码中文时，UTF-16 占用的空间比 UTF-8 更小。Java 和 C# 等编程语言默认使用 UTF-16 编码。

#### Q & A

**Q**：为什么哈希表同时包含线性数据结构和非线性数据结构？

哈希表底层是数组，而为了解决哈希冲突，我们可能会使用“链式地址”（后续“哈希冲突”章节会讲）：数组中每个桶指向一个链表，当链表长度超过一定阈值时，又可能被转化为树（通常为红黑树）。

从存储的角度来看，哈希表的底层是数组，其中每一个桶槽位可能包含一个值，也可能包含一个链表或一棵树。因此，哈希表可能同时包含线性数据结构（数组、链表）和非线性数据结构（树）。

**Q**：`char` 类型的长度是 1 字节吗？

`char` 类型的长度由编程语言采用的编码方法决定。例如，Java、JavaScript、TypeScript、C# 都采用 UTF-16 编码（保存 Unicode 码点），因此 `char` 类型的长度为 2 字节。

**Q**：基于数组实现的数据结构也称“静态数据结构” 是否有歧义？栈也可以进行出栈和入栈等操作，这些操作都是“动态”的。

栈确实可以实现动态的数据操作，但数据结构仍然是“静态”（长度不可变）的。尽管基于数组的数据结构可以动态地添加或删除元素，但它们的容量是固定的。如果数据量超出了预分配的大小，就需要创建一个新的更大的数组，并将旧数组的内容复制到新数组中。

**Q**：在构建栈（队列）的时候，未指定它的大小，为什么它们是“静态数据结构”呢？

在高级编程语言中，我们无须人工指定栈（队列）的初始容量，这个工作由类内部自动完成。例如，Java 的 `ArrayList` 的初始容量通常为 10。另外，扩容操作也是自动实现的。详见后续的“列表”章节。



















